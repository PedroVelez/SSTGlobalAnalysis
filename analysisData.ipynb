{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0037c146-943a-43df-b5d6-d1d0b5a8189a",
   "metadata": {},
   "source": [
    "# Global mean Sea Surface Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df62f37-e43c-4f83-9107-5cebfdbacca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from matplotlib.dates import DateFormatter\n",
    "import platform\n",
    "import os\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import dask\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.ops import transform\n",
    "import pyproj\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075c2b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>---------------------------------------------------------------------------<<<<<\n",
      "    > Linux rossby 6.8.0-58-generic #60-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18\n",
      "    >\n",
      "    > Home Dir  - /home/pvb\n",
      "    > data      /data/pvb\n",
      "    > Proyectos - /home/pvb/Proyectos\n",
      "    > Analisis  - /home/pvb/Analisis\n",
      "    > Data      - /data/pvb\n",
      "    > Argo data - /data/pvb/Argo\n",
      ">>>>>---------------------------------------------------------------------------<<<<<\n"
     ]
    }
   ],
   "source": [
    "from globales import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7167f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "GlobalSU['DatPath'] = \"Your value here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a0cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation to ensure the polygon's CRS matches\n",
    "# Transform the polygon to match the DataArray CRS if needed\n",
    "def transform_polygon(polygon, src_crs='epsg:4326', tgt_crs='epsg:4326'):\n",
    "    proj = pyproj.Transformer.from_proj(pyproj.Proj(src_crs), pyproj.Proj(tgt_crs), always_xy=True)\n",
    "    return transform(lambda x, y: proj.transform(x, y), polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc159cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_in_polygon(lon, lat, polygon):\n",
    "    point = Point(lon, lat)\n",
    "    return polygon.contains(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e010352-6479-443b-8466-06aac184f261",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fa8168-9a5e-450d-ac67-7e24fe1fad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "year1= 1982\n",
    "year2= 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95fc918c-c177-4c1c-bee5-efd9e7fe07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings compute de climatoloy\n",
    "yearC1='1982'\n",
    "yearC2='1992'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769bb64d",
   "metadata": {},
   "source": [
    "# Inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d11420d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME=os.environ['HOME']   \n",
    "f = open(HOME+'/.env', 'r')\n",
    "for line in f.readlines():\n",
    "    Name=line.strip().split('=')[0]\n",
    "    Content=line.strip().split('=')[-1]\n",
    "    if Name=='dirData' or Name=='dirAnalisis':\n",
    "        exec(Name + \"=\" + \"'\" + Content + \"'\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7a97b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.uname().nodename.lower().find('eemmmbp') != -1:\n",
    "    base_file = dirData + '/Satelite/noaa.oisst.v2.highres/NC/sst.day.mean'\n",
    "    dataDir   = dirAnalisis + '/SSTGlobalAnalysis/data'\n",
    "elif os.uname().nodename.lower().find('sagams') != -1:\n",
    "    base_file = dirData + '/Satelite/noaa.oisst.v2.highres/NC/sst.day.mean'\n",
    "    dataDir   = dirAnalisis + '/SSTGlobalAnalysis/data'\n",
    "elif os.uname().nodename.lower().find('rossby') != -1:\n",
    "    base_file = dirData + '/Satelite/noaa.oisst.v2.highres/NC/sst.day.mean'\n",
    "    dataDir   = dirAnalisis + '/SSTGlobalAnalysis/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fad8099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/pvb/Satelite/noaa.oisst.v2.highres/NC/sst.day.mean'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c26178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Titulos = ['Oceano Global','Hemisferio norte','Hemisferio sur',\n",
    "           'Atlántico Norte','Iberian Canary Basin',\n",
    "           'Demarcación marina levantino-balear', 'Demarcación marina noratlántica','Demarcación marina canaria','Demarcación sudatlántica','Demarcación Estrecho y Alborán']\n",
    "Titulos_short = ['GO','NH','SH',\n",
    "                 'NAtl','IBICan',\n",
    "                 'LEB', 'NOR','CAN','SUD','ESA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87781c5c-5ee1-4068-9962-e9cef21a2291",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33e222c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "it=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a29d8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = Titulos[it]\n",
    "titulo_short = Titulos_short[it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53a62410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demarcación marina levantino-balear\n"
     ]
    }
   ],
   "source": [
    "print(titulo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "609a9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f'{base_file}.{year}.nc' for year in range(year1, year2+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d63a86b-a734-46b6-80eb-d8f0545badfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = xr.open_mfdataset(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6ec24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the .txt file\n",
    "demCoord = []\n",
    "longDem, latiDem = [], []\n",
    "with open('./data/Demarcacion'+titulo_short+'.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        # Split the line by whitespace and append the values\n",
    "        longitude, latitude = map(float, line.split())\n",
    "        longitude=longitude+360\n",
    "        longDem.append(longitude)\n",
    "        latiDem.append(latitude)\n",
    "        demCoord.append((longitude,latitude))\n",
    "demPolygon = Polygon(demCoord)    \n",
    "demPolygon_transformed = transform_polygon(demPolygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54958a4a-4330-4f2a-aff1-f4b3eda57a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Demarcación marina levantino-balear\n"
     ]
    }
   ],
   "source": [
    "if titulo_short == 'LEB':\n",
    "    slicelatitude=slice(35.5,42.75)\n",
    "    slicelongitude=slice(358,368)\n",
    "    sst=DS.sst.sel(lat=slicelatitude).sel(lon=slicelongitude)\n",
    "    mask = np.array([[point_in_polygon(lon,lat,demPolygon_transformed) \n",
    "         for lon in sst.lon.values] \n",
    "         for lat in sst.lat.values])\n",
    "    sst_unmasked = sst\n",
    "    sst = sst.where(mask)\n",
    "    print('>>>>> '+titulo)        \n",
    "\n",
    "elif  titulo_short == 'NOR':\n",
    "    slicelatitude=slice(41.5,46.9)\n",
    "    slicelongitude=slice(346,360)\n",
    "    sst=DS.sst.sel(lat=slicelatitude).sel(lon=slicelongitude)\n",
    "    mask = np.array([[point_in_polygon(lon,lat,demPolygon_transformed) \n",
    "         for lon in sst.lon.values] \n",
    "         for lat in sst.lat.values])\n",
    "    sst_unmasked = sst\n",
    "    sst = sst.where(mask)\n",
    "    print('>>>>> '+titulo)        \n",
    "        \n",
    "elif  titulo_short == 'CAN':\n",
    "    slicelatitude=slice(24,32.5)\n",
    "    slicelongitude=slice(335,350)\n",
    "    sst=DS.sst.sel(lat=slicelatitude).sel(lon=slicelongitude)\n",
    "    mask = np.array([[point_in_polygon(lon,lat,demPolygon_transformed) \n",
    "         for lon in sst.lon.values] \n",
    "         for lat in sst.lat.values])\n",
    "    sst_unmasked = sst\n",
    "    sst = sst.where(mask)\n",
    "    print('>>>>> '+titulo)    \n",
    "\n",
    "elif  titulo_short == 'SUD':\n",
    "    slicelatitude=slice(35.5,37.5)\n",
    "    slicelongitude=slice(352,354.5)\n",
    "    sst=DS.sst.sel(lat=slicelatitude).sel(lon=slicelongitude)\n",
    "    mask = np.array([[point_in_polygon(lon,lat,demPolygon_transformed) \n",
    "         for lon in sst.lon.values] \n",
    "         for lat in sst.lat.values])\n",
    "    sst_unmasked = sst\n",
    "    sst = sst.where(mask)\n",
    "    print('>>>>> '+titulo)\n",
    "\n",
    "elif  titulo_short == 'ESA':\n",
    "    slicelatitude=slice(35.5,37)\n",
    "    slicelongitude=slice(354,358.5)\n",
    "    sst=DS.sst.sel(lat=slicelatitude).sel(lon=slicelongitude)\n",
    "    mask = np.array([[point_in_polygon(lon,lat,demPolygon_transformed) \n",
    "         for lon in sst.lon.values] \n",
    "         for lat in sst.lat.values])\n",
    "    sst_unmasked = sst\n",
    "    sst = sst.where(mask)\n",
    "    print('>>>>> '+titulo)\n",
    "elif  titulo_short == 'IBICan':\n",
    "     #sst = DS.sst.sel(lat=slice(20, 50),lon=slice(325,360))\n",
    "     #basins = xr.open_dataset(dataDir+'/basins.nc')\n",
    "     #basin_surf = basins.basin[0]\n",
    "     #basin_surf_interp = basin_surf.interp_like(sst, method='nearest')\n",
    "     #sst = sst.where((basin_surf_interp==1) ,drop=True)\n",
    "     sst = DS.sst.sel(lat=slice(20, 47),lon=slice(325,360))\n",
    "     # Para blanquear el mediterraneo\n",
    "     lat_point_list = [40, 40, 30, 30, 40]\n",
    "     lon_point_list = [354.5, 360, 360, 354.5, 354.5]\n",
    "     polygon_geom = Polygon(zip(lon_point_list, lat_point_list))\n",
    "     polygon = transform_polygon(polygon_geom)\n",
    "     mask = np.array([[point_in_polygon(lon,lat,polygon) \n",
    "                  for lon in sst.lon.values] \n",
    "                  for lat in sst.lat.values])    \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c48e104c-74f0-42da-b07f-59e2c92d6adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> Demarcación marina canaria\n"
     ]
    }
   ],
   "source": [
    "print('>>>>> '+titulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5526e-56d0-43a4-be11-034704a691d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Daily analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61a2b504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>> DailyDemarcación marina levantino-balearLEB\n"
     ]
    }
   ],
   "source": [
    "print('>>>>> Daily'+titulo+titulo_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "571f7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate mean weigthtened\n",
    "weights = np.cos(np.deg2rad(sst.lat))\n",
    "weights = weights/weights.max()\n",
    "weights.name = \"weights\"\n",
    "sst_weighted = sst.weighted(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f96b0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_wmean = sst_weighted.mean((\"lon\", \"lat\"),skipna=True).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22d9dedd-dbf1-4a6d-9b0b-0b0a86bce239",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create monthly climatology\n",
    "sst_clim = sst.sel(time=slice(yearC1,yearC2)).groupby('time.dayofyear').mean(dim='time').load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700dd9c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c665722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ocean/lib/python3.11/site-packages/xarray/core/indexing.py:1593: PerformanceWarning: Slicing with an out-of-order index is generating 44 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "## Create anomaly\n",
    "sst_anom = sst.groupby('time.dayofyear') - sst_clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "160a7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate global mean anomaly\n",
    "weights = np.cos(np.deg2rad(sst.lat))\n",
    "weights = weights/weights.max()\n",
    "weights.name = \"weights\"\n",
    "sst_anom_weighted = sst_anom.weighted(weights)\n",
    "sst_anom_wmean = sst_anom_weighted.mean((\"lon\", \"lat\"),skipna=True).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fcb238db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save in netcdf\n",
    "sst_wmean.to_netcdf(dataDir+'/sstd_mean_'+titulo_short+'.nc',mode='w')   \n",
    "sst_anom_wmean.to_netcdf(dataDir+'/sstd_anom_mean_'+titulo_short+'.nc',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "748bbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if titulo_short=='GO' or titulo_short=='NAtl' or titulo_short=='LEB' or titulo_short=='CAN' or titulo_short=='NOR' or titulo_short=='SUD' or titulo_short=='ESA' or titulo_short=='IBICan':\n",
    "        sst_anom_LD=sst_anom[-1,:,:]\n",
    "        sst_anom_LD.to_netcdf(dataDir+'/sstLD_anom_'+titulo_short+'.nc',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee496a",
   "metadata": {},
   "source": [
    "# Monthly analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2e8f5dc-178e-4f40-9cd0-c369b86f86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst = sst.resample(time='ME').mean(dim='time',skipna=True).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a3ed02d-e627-4d65-80cb-47e3855fe547",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate global mean weigthtened\n",
    "weights = np.cos(np.deg2rad(sst.lat))\n",
    "weights = weights/weights.max()\n",
    "weights.name = \"weights\"\n",
    "sst_weighted = sst.weighted(weights)\n",
    "sst_wmean = sst_weighted.mean((\"lon\", \"lat\"),skipna=True).load()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dfd2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create monthly climatology\n",
    "sst_clim = sst.sel(time=slice(yearC1,yearC2)).groupby('time.month').mean(dim='time').load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b62f5a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > Compute anomaly mean\n"
     ]
    }
   ],
   "source": [
    "## Create anomaly\n",
    "print('    > Compute anomaly mean')\n",
    "sst_anom = sst.groupby('time.month') - sst_clim\n",
    "sst_anom.load();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c30b787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > Compute weigthtened mean\n"
     ]
    }
   ],
   "source": [
    "##Calculate global mean weigthtened\n",
    "print('    > Compute weigthtened mean')\n",
    "weights = np.cos(np.deg2rad(sst.lat))\n",
    "weights = weights/weights.max()\n",
    "weights.name = \"weights\"\n",
    "sst_anom_weighted = sst_anom.weighted(weights)\n",
    "sst_anom_wmean = sst_anom_weighted.mean((\"lon\", \"lat\"),skipna=True).load()\n",
    "sst_anom_wmean_rolling = sst_anom_wmean.rolling(time=12,center=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Save in netcdf\n",
    "print('    > to netcdf')\n",
    "sst_anom.to_netcdf(dataDir+'/sstm_anom_'+titulo_short+'.nc',mode='w')\n",
    "sst_wmean.to_netcdf(dataDir+'/sstm_mean_'+titulo_short+'.nc',mode='w')\n",
    "sst_anom_wmean.to_netcdf(dataDir+'/sstm_anom_mean_'+titulo_short+'.nc',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e85fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
